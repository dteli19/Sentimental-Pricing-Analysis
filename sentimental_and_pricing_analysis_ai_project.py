# -*- coding: utf-8 -*-
"""Sentimental And Pricing Analysis AI Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s2nrUaOp5fvCyZ1cQrb6TBlHMom_45Px

# Rhode Island Airbnb Listings — NLP, Sentiment & Forecasting

## Project Overview

The rapid growth of short-term rental platforms such as Airbnb has transformed the hospitality landscape, creating new opportunities for both hosts and travelers. However, customer satisfaction and booking decisions are heavily influenced by online reviews, which contain rich textual insights that go beyond numeric ratings.

This project focuses on Airbnb listings and guest reviews in **Rhode Island**, combining tabular listing data with **Natural Language Processing (NLP)**–based sentiment analysis on written reviews and a **forecasting model** of listing satisfaction.

We aim to:

- Clean and analyze **guest reviews** using NLP and **VADER sentiment analysis**.
- Explore **key themes and word usage** in positive vs. negative reviews.
- Integrate **listing-level features** (price, property type, location, amenities, host status) with aggregated review sentiment.
- Build a **forecasting model** that predicts **listing-level satisfaction** (Airbnb’s `review_scores_rating`) from listing attributes and text-derived sentiment features.
- Evaluate the model using **train–test split, Mean Absolute Error (MAE), and R²**.

**Guiding question:**

> *“What do Airbnb guests in Rhode Island value most—and complain about most—based on the sentiment embedded in their written reviews, and how well can we forecast listing satisfaction from listing characteristics and review sentiment?”*
"""

# Install extra libraries (run this once per runtime)
!pip install vaderSentiment wordcloud bs4

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from collections import Counter
import re

# For HTML cleaning
from bs4 import BeautifulSoup

# For NLP (tokenization, stopwords)
import nltk
from nltk.corpus import stopwords

# For word cloud
from wordcloud import WordCloud

# For sentiment analysis
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# For interactive maps
import plotly.express as px

# For model building
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

from IPython.display import display

# Aesthetics
sns.set(style="whitegrid")
plt.rcParams["figure.dpi"] = 120

# Fix randomness
np.random.seed(42)

# Download NLTK resources
nltk.download("punkt")
nltk.download("stopwords")

"""## 2. Load Airbnb Datasets (Listings & Reviews)

We use InsideAirbnb’s publicly available data for **Rhode Island** (2025-06-28 snapshot):

- `listings.csv.gz`: property-level information (price, location, host details, amenities, review scores)
- `reviews.csv.gz`: guest reviews with dates and comments

"""

# URLs for InsideAirbnb Rhode Island data
reviews_url = "https://data.insideairbnb.com/united-states/ri/rhode-island/2025-06-28/data/reviews.csv.gz"
listings_url = "https://data.insideairbnb.com/united-states/ri/rhode-island/2025-06-28/data/listings.csv.gz"

# Load data
reviews_df = pd.read_csv(reviews_url, compression="gzip")
listings_df = pd.read_csv(listings_url, compression="gzip")

print("Reviews shape:", reviews_df.shape)
print("Listings shape:", listings_df.shape)

"""## 3. Basic Sanity Checks

We inspect:

- First few rows
- Column names
- Missing values
"""

display(reviews_df.head())
display(listings_df.head())

print("\nReviews columns:\n", reviews_df.columns)
print("\nListings columns:\n", listings_df.columns)

print("\nMissing values in reviews:")
display(reviews_df.isna().sum())

print("\nMissing values in listings:")
display(listings_df.isna().sum())

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming your dataframe is called ri_df
missing_counts = ri_df.isnull().sum()
missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=missing_counts.values, y=missing_counts.index)
plt.title("Missing Values by Column", fontsize=14)
plt.xlabel("Number of Missing Entries")
plt.ylabel("Columns")
plt.tight_layout()
plt.show()

"""## 4. Clean Nulls in Reviews

We drop rows with:

- Missing `comments`
- Missing `reviewer_name`

These do not contribute useful information to text-based analysis.

"""

reviews_df = reviews_df.dropna(subset=["comments", "reviewer_name"])
reviews_df.isna().sum()

"""## 5. Listings EDA — Focus on Rhode Island

Although the market snapshot is for Rhode Island, some host locations may fall outside the state. We:

1. Explore `host_location`.
2. Filter listings where `host_location` contains **“RI”** or **“Rhode Island”**.
3. Keep a Rhode Island–focused subset and enrich it with additional features for modeling.
"""

unique_locations = listings_df["host_location"].unique()
print("Number of unique host locations:", len(unique_locations))
print("Sample locations:", unique_locations[:40])

# Filter listings for Rhode Island (non-capturing regex group to avoid warnings)
ri_df = listings_df[
    listings_df["host_location"].str.contains(
        r"\b(?:RI|Rhode\s*Island)\b", case=False, na=False
    )
].copy()

ri_df.shape

print("Sample locations:", ri_df["host_location"].unique()[:40])

"""## 6. Select Relevant Listing Features & Handle Missing Values

We keep key features that will later be used in the **forecasting model**:

- Listing ID, price, bathrooms, bedrooms, accommodates
- Number of reviews
- Location (latitude, longitude, neighbourhood)
- Room type & property type
- Host features: identity verified, superhost
- Amenities (to derive an `amenities_count`)
- `review_scores_rating` (our target variable for satisfaction forecasting)
"""

selected_columns = [
    "id",
    "price",
    "bathrooms",
    "bedrooms",
    "accommodates",
    "number_of_reviews",
    "latitude",
    "longitude",
    "room_type",
    "property_type",
    "neighbourhood_cleansed",
    "host_identity_verified",
    "host_is_superhost",
    "amenities",
    "review_scores_rating",
]

ri_df = ri_df[selected_columns].copy()
ri_df = ri_df.rename(columns={"id": "listing_id"})

ri_df.head()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Assuming your dataframe is called ri_df
missing_counts = ri_df.isnull().sum()
missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=missing_counts.values, y=missing_counts.index)
plt.title("Missing Values by Column", fontsize=14)
plt.xlabel("Number of Missing Entries")
plt.ylabel("Columns")
plt.tight_layout()
plt.show()

# Drop rows where price or review_scores_rating is missing (needed for modeling)
ri_df = ri_df.dropna(subset=["price", "review_scores_rating"])

# Impute bathrooms & bedrooms if needed
if ri_df["bathrooms"].isna().sum() > 0:
    ri_df["bathrooms"] = ri_df["bathrooms"].fillna(ri_df["bathrooms"].mean())

if ri_df["bedrooms"].isna().sum() > 0:
    ri_df["bedrooms"] = ri_df["bedrooms"].fillna(ri_df["bedrooms"].median())

# Derive amenities_count from amenities text
ri_df["amenities"] = ri_df["amenities"].fillna("")
ri_df["amenities_count"] = ri_df["amenities"].str.count(",") + 1

ri_df.isnull().sum()

"""## 7. Type Conversion & Encoding (Listings)

- Convert `price` to numeric.
- Convert boolean-like columns to 1/0.
- Keep `room_type` & `property_type` as categorical (for modeling) and optionally create dummies for descriptive analysis.
"""

# Convert price to numeric
ri_df["price"] = ri_df["price"].replace(r"[\$,]", "", regex=True).astype(float)

# Map t/f to 1/0 for host flags
ri_df["host_identity_verified"] = ri_df["host_identity_verified"].map({"t": 1, "f": 0})
ri_df["host_is_superhost"] = ri_df["host_is_superhost"].map({"t": 1, "f": 0})

# One-hot encode room_type for some EDA (but keep original room_type column)
room_type_dummies = pd.get_dummies(
    ri_df["room_type"], prefix="room_type", drop_first=True
).astype(int)

ri_df = pd.concat([ri_df, room_type_dummies], axis=1)

ri_df.head()

"""## 8. Listings Exploratory Data Analysis (EDA)
### 8.1 Descriptive Statistics
"""

ri_df.describe(include="all")

"""### 8.2 Distributions of Key Numeric Variables"""

numeric_cols = ["price", "bedrooms", "bathrooms", "accommodates", "number_of_reviews"]
ri_df[numeric_cols].hist(bins=30, figsize=(10, 8))
plt.tight_layout()
plt.show()

"""### 8.3 Correlation Matrix"""

plt.figure(figsize=(10, 8))
ri_numeric = ri_df[
    [
        "price",
        "bathrooms",
        "bedrooms",
        "accommodates",
        "number_of_reviews",
        "host_identity_verified",
        "host_is_superhost",
        "amenities_count",
        "review_scores_rating",
    ]
]

corr = ri_numeric.corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", annot_kws={"size": 8})
plt.xticks(rotation=45, ha="right", fontsize=8)
plt.yticks(rotation=0, fontsize=8)
plt.title("Correlation Matrix (Rhode Island Listings)")
plt.tight_layout()
plt.show()

"""### 8.4 Handling Price Outliers & Log-Transform"""

plt.figure(figsize=(10, 4))
sns.histplot(ri_df["price"], bins=50, kde=True, color="green")
plt.title("Price Distribution (Before Removing Top 1%)")
plt.xlabel("Price")
plt.show()

# Remove top 1% price outliers
price_cap = ri_df["price"].quantile(0.99)
ri_df_clean = ri_df[ri_df["price"] <= price_cap].copy()

plt.figure(figsize=(10, 4))
sns.histplot(ri_df["price"], bins=50, kde=True, color="green")
plt.title("Price Distribution (After Removing Top 1%)")
plt.xlabel("Price")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))

# Combine prices into a comparison-friendly dataframe
box_df = pd.DataFrame({
    "Original": ri_df["price"],
    "Cleaned": ri_df_clean["price"]
})

sns.boxplot(data=box_df)
plt.title("Price Distribution Before vs After Outlier Removal", fontsize=14)
plt.ylabel("Price")
plt.tight_layout()
plt.show()

# Log-transform price
ri_df["log_price"] = np.log1p(ri_df["price"])

plt.figure(figsize=(10, 4))
sns.histplot(ri_df["log_price"], bins=50, kde=True)
plt.title("Log-Transformed Price")
plt.xlabel("log(1 + price)")
plt.show()

"""### 8.5 Price Ranges & Geospatial Map"""

ri_df["price_range"] = pd.cut(
    ri_df["price"],
    bins=[0, 100, 200, 300, ri_df["price"].max()],
    labels=["< $100", "$100–$200", "$200–$300", ">$300"],
)

fig = px.scatter_mapbox(
    ri_df,
    lat="latitude",
    lon="longitude",
    color="price_range",
    hover_data=["price", "bedrooms", "bathrooms"],
    zoom=10,
    mapbox_style="open-street-map",
    title="Airbnb Listings in Rhode Island by Price Range",
)
fig.show()

"""## 9. Reviews EDA & Text Preprocessing

We now focus on the **reviews dataset**, performing:

1. Date conversion.
2. HTML/tag removal and whitespace normalization.
3. Tokenization + stopword removal.
4. Review length analysis.
5. Word cloud visualization.
"""

# Convert date to datetime
reviews_df["date"] = pd.to_datetime(reviews_df["date"])

# Basic whitespace cleaning
def basic_clean_text(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s

reviews_df["comments"] = reviews_df["comments"].astype(str).map(basic_clean_text)

# Remove HTML tags/entities
def clean_raw_comment(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = BeautifulSoup(text, "html.parser").get_text(separator=" ")
    text = re.sub(r"&[a-z]+;", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

reviews_df["comments"] = reviews_df["comments"].astype(str).map(clean_raw_comment)

# Tokenization + stopword removal
stop_words = set(stopwords.words("english"))

def clean_text(text: str) -> str:
    text = text.lower()
    tokens = nltk.RegexpTokenizer(r"\b[a-z]{2,}\b").tokenize(text)
    tokens = [w for w in tokens if w not in stop_words]
    return " ".join(tokens)

reviews_df["clean_text"] = reviews_df["comments"].apply(clean_text)
reviews_df[["comments", "clean_text"]].head()

reviews_df.shape, reviews_df.isna().sum()

"""### 9.1 Review Length Analysis"""

reviews_df["word_count"] = reviews_df["clean_text"].apply(lambda x: len(x.split()))
reviews_df["char_count"] = reviews_df["comments"].apply(len)

fig, ax = plt.subplots(1, 2, figsize=(12, 4))

sns.histplot(reviews_df["word_count"], bins=50, ax=ax[0])
ax[0].set_title("Distribution of Review Word Counts")
ax[0].set_xlabel("Word Count")

sns.boxplot(x=reviews_df["word_count"], ax=ax[1])
ax[1].set_title("Boxplot of Review Length (Words)")
ax[1].set_xlabel("Word Count")

plt.tight_layout()
plt.show()

print("Average words per review:", reviews_df["word_count"].mean())

"""### 9.2 Word Cloud"""

all_text = " ".join(reviews_df["clean_text"])

wordcloud = WordCloud(
    width=1000, height=600, background_color="white", max_words=100, colormap="viridis"
).generate(all_text)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Top Words in Rhode Island Airbnb Reviews")
plt.show()

"""## 10. Sentiment Analysis with VADER

We compute a **compound score** in [-1, 1] per review and label each as:

- Positive (`score ≥ 0.05`)
- Negative (`score ≤ -0.05`)
- Neutral (otherwise)
"""

analyzer = SentimentIntensityAnalyzer()

reviews_df["vader_score"] = reviews_df["comments"].apply(
    lambda x: analyzer.polarity_scores(str(x))["compound"]
)

def label_sentiment(score: float) -> str:
    if score >= 0.05:
        return "Positive"
    elif score <= -0.05:
        return "Negative"
    else:
        return "Neutral"

reviews_df["sentiment"] = reviews_df["vader_score"].apply(label_sentiment)
reviews_df["sentiment"].value_counts(normalize=True).round(3)

sns.countplot(
    data=reviews_df,
    x="sentiment",
    order=["Positive", "Neutral", "Negative"],
    palette="Set2",
)
plt.title("Sentiment Distribution of Rhode Island Airbnb Reviews")
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
plt.show()

"""### 10.1 Sentiment Over Time"""

sentiment_trend = (
    reviews_df.groupby("date")["vader_score"].mean().reset_index().sort_values("date")
)

plt.figure(figsize=(10, 4))
sns.lineplot(data=sentiment_trend, x="date", y="vader_score", color="green")
plt.title("Average Sentiment Score Over Time")
plt.xlabel("Date")
plt.ylabel("Average VADER Compound Score")
plt.tight_layout()
plt.show()

"""### 10.2 Most Common Positive & Negative Words"""

positive_words = " ".join(
    reviews_df[reviews_df["sentiment"] == "Positive"]["clean_text"]
).split()
negative_words = " ".join(
    reviews_df[reviews_df["sentiment"] == "Negative"]["clean_text"]
).split()

pos_freq = Counter(positive_words)
neg_freq = Counter(negative_words)

pos_df = pd.DataFrame(pos_freq.most_common(20), columns=["Word", "Frequency"])
neg_df = pd.DataFrame(neg_freq.most_common(20), columns=["Word", "Frequency"])

print("Top Positive Words:")
display(pos_df.head(10))

print("Top Negative Words:")
display(neg_df.head(10))

print("% of Positive Reviews:", round((reviews_df["sentiment"] == "Positive").mean() * 100, 2))
print("% of Negative Reviews:", round((reviews_df["sentiment"] == "Negative").mean() * 100, 2))
print("Neutral share:", round((reviews_df["sentiment"] == "Neutral").mean() * 100, 2))

print("\nTop Positive Words:", [w for w, f in pos_freq.most_common(10)])
print("Top Negative Words:", [w for w, f in neg_freq.most_common(10)])

"""## 11. Combine Listings and Reviews

To connect **review sentiment** with **listing characteristics**, we join:

- `ri_df` (Rhode Island listings with price, amenities, property info, review scores)
- `reviews_df` (all Rhode Island reviews with VADER sentiment)

on the common `listing_id` key.

We then aggregate sentiment at the listing level.
"""

print("Reviews columns:", reviews_df.columns)
print("RI listings columns:", ri_df.columns)

# reviews_df has listing_id already; ri_df uses listing_id too
final_df = pd.merge(ri_df, reviews_df, on="listing_id", how="inner")
final_df.shape

final_df.head()

"""### 11.1 Listing-Level Sentiment Aggregation (`summary_df`)

For each listing, we compute:

- `avg_vader_score`: mean sentiment across reviews.
- `most_common_sentiment`: dominant sentiment category.
- Keep `price_range`, `latitude`, `longitude` for plotting.
"""

summary_df = (
    final_df.groupby("listing_id")
    .agg(
        price_range=("price_range", lambda x: x.mode()[0]),
        latitude=("latitude", "first"),
        longitude=("longitude", "first"),
        avg_vader_score=("vader_score", "mean"),
        most_common_sentiment=("sentiment", lambda x: x.value_counts().idxmax()),
    )
    .reset_index()
)

summary_df.head()

plt.figure(figsize=(10, 6))

sns.boxplot(
    data=summary_df,
    x="price_range",
    y="avg_vader_score",
    palette="viridis"
)

plt.title("Sentiment Score Distribution Across Price Ranges")
plt.xlabel("Price Range")
plt.ylabel("VADER Sentiment Score")
plt.xticks(rotation=45)
plt.show()

ri_df["price_range"].value_counts()

"""## 12. Sentiment vs Price & Location (Visualization)"""

plt.figure(figsize=(8, 6))
sns.countplot(
    data=summary_df,
    x="price_range",
    hue="most_common_sentiment",
    palette="Set2",
)
plt.title("Sentiment Distribution Across Price Ranges")
plt.xlabel("Price Range")
plt.ylabel("Number of Listings")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

fig = px.scatter_mapbox(
    summary_df,
    lat="latitude",
    lon="longitude",
    color="most_common_sentiment",
    hover_name="listing_id",
    hover_data=["price_range", "avg_vader_score"],
    zoom=9,
    mapbox_style="open-street-map",
    title="Airbnb Listings in Rhode Island by Dominant Sentiment",
)
fig.show()

plt.figure(figsize=(6, 4))
summary_df["most_common_sentiment"].value_counts().plot(kind="bar")
plt.title("Most Common Sentiment Across Listings")
plt.xlabel("Sentiment")
plt.ylabel("Number of Listings")
plt.tight_layout()
plt.show()

df_sorted = summary_df.sort_values("avg_vader_score", ascending=False)

plt.figure(figsize=(10, 8))
plt.hlines(
    y=df_sorted["listing_id"], xmin=0, xmax=df_sorted["avg_vader_score"], color="gray"
)
plt.scatter(
    df_sorted["avg_vader_score"], df_sorted["listing_id"], color="darkgreen", s=40
)
plt.title("Average VADER Score per Listing")
plt.xlabel("Average VADER Score")
plt.ylabel("Listing ID")
plt.tight_layout()
plt.show()

"""## 13. Extra Insight: Sentiment by Room Type & Superhost

These views connect **what guests say** with **how listings are structured**.
"""

# Create a simple room_type label using the original column
final_df["room_type_label"] = final_df["room_type"]

# Sentiment by Superhost
sentiment_superhost = (
    final_df.groupby(["host_is_superhost", "sentiment"])["vader_score"]
    .count()
    .reset_index(name="count")
)

plt.figure(figsize=(8, 5))
sns.barplot(
    data=sentiment_superhost,
    x="host_is_superhost",
    y="count",
    hue="sentiment",
    palette="Set2",
)
plt.title("Sentiment Distribution by Superhost Status")
plt.xlabel("Host is Superhost (0 = No, 1 = Yes)")
plt.ylabel("Number of Reviews")
plt.tight_layout()
plt.show()

# Sentiment by room type
sentiment_roomtype = (
    final_df.groupby(["room_type_label", "sentiment"])["vader_score"]
    .count()
    .reset_index(name="count")
)

plt.figure(figsize=(8, 5))
sns.barplot(
    data=sentiment_roomtype,
    x="room_type_label",
    y="count",
    hue="sentiment",
    palette="Set2",
)
plt.title("Sentiment Distribution by Room Type")
plt.xlabel("Room Type")
plt.ylabel("Number of Reviews")
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

"""## 14. Forecasting Model: Predicting Listing Satisfaction

The assignment requires:

- Using at least **two datasets** (we combine **listings** + **reviews**).
- A **forecasting / predictive model**.

Here, we build a model to forecast **listing-level satisfaction**, measured by Airbnb’s `review_scores_rating` (0–100), using:

- **Listing features** from `listings_df`:
  - `log_price`, `bedrooms`, `bathrooms`, `accommodates`, `number_of_reviews`
  - `amenities_count`
  - `host_identity_verified`, `host_is_superhost`
  - `property_type`, `room_type`, `neighbourhood_cleansed`
- **Text-derived sentiment features** from `reviews_df`:
  - `avg_vader_score` per listing

### 14.1 Build Modeling Dataset

We merge `summary_df` (listing-level sentiment) back into the enriched `ri_df` (listing-level attributes).
"""

# Merge listing-level sentiment into listing feature table
# ri_df has one row per listing; summary_df also has one row per listing_id
model_df = pd.merge(
    ri_df,
    summary_df[["listing_id", "avg_vader_score"]],
    on="listing_id",
    how="inner",
)

# Keep only rows with non-null target
model_df = model_df.dropna(subset=["review_scores_rating", "avg_vader_score"])

model_df.head()

model_df.shape

"""### 14.2 Define Features and Target

- **Target (`y`)**: `review_scores_rating`  
- **Numeric features**:
  - `log_price`, `bedrooms`, `bathrooms`, `accommodates`
  - `number_of_reviews`, `amenities_count`
  - `host_identity_verified`, `host_is_superhost`
  - `avg_vader_score`
- **Categorical features**:
  - `property_type`, `room_type`, `neighbourhood_cleansed`
"""

target = "review_scores_rating"

numeric_features = [
    "log_price",
    "bedrooms",
    "bathrooms",
    "accommodates",
    "number_of_reviews",
    "amenities_count",
    "host_identity_verified",
    "host_is_superhost",
    "avg_vader_score",
]

categorical_features = ["property_type", "room_type", "neighbourhood_cleansed"]

X = model_df[numeric_features + categorical_features]
y = model_df[target]

"""### 14.3 Train–Test Split and Pipeline

We:

1. Split data into **train (80%)** and **test (20%)** sets.
2. Use a `ColumnTransformer`:
   - Pass numeric features through unchanged.
   - Apply `OneHotEncoder` to categorical features.
3. Fit a **RandomForestRegressor** to predict `review_scores_rating`.
4. Evaluate with **Mean Absolute Error (MAE)** and **R²** on the test set.
"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

# Numeric transformer: impute median
numeric_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="median"))
    ]
)

# Categorical transformer: impute mode + one-hot encode
categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ]
)

# Combine preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# Define model
rf_model = RandomForestRegressor(n_estimators=200, random_state=42)

# Full pipeline
forecast_pipeline = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("model", rf_model)
    ]
)

# Train–test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Fit model
forecast_pipeline.fit(X_train, y_train)

# Predict
y_pred = forecast_pipeline.predict(X_test)

# Metrics
print("Test MAE:", round(mean_absolute_error(y_test, y_pred), 2))
print("Test R²:", round(r2_score(y_test, y_pred), 3))

"""### 14.4 Interpretation of Forecasting Results

- **Model choice**: Random Forest is a flexible, non-linear ensemble model that can capture complex interactions between listing features and sentiment while handling mixed numeric and categorical inputs.
- **Input variables**:
  - *Listing characteristics* (price, size, amenities, host status, location).
  - *Text-derived sentiment* (`avg_vader_score`) summarizing guest feedback.
- **Evaluation strategy**:
  - A **hold-out test set (20%)** was used for unbiased evaluation.
  - **Mean Absolute Error (MAE)** quantifies the average error between predicted and actual ratings.
  - **R²** measures the proportion of variance in `review_scores_rating` explained by the model.

In practice, an MAE of a few rating points and a moderate-to-high R² would indicate that:
- Listings with higher **sentiment scores**, more **amenities**, and positive host attributes (e.g., superhost, identity verified) tend to have **higher satisfaction ratings**.
- Location and property type also contribute to differences in expected review scores.

This forecasting model directly addresses the requirement to **integrate multiple datasets** (listings + reviews) and demonstrates how textual sentiment signals can be combined with structured listing features to **predict guest satisfaction**.
"""